# ML_Performance_Metrics
**Accuracy** measures the percentage of correct predictions made by a model out of all predictions made. It provides an overall assessment of the model's correctness, making it a commonly used metric for evaluating machine learning models across various applications.

**Precision** evaluates the proportion of true positive predictions among all positive predictions made by the model. It is particularly useful in scenarios where minimizing false positives is crucial, such as medical diagnostics or fraud detection systems.

**Recall**, also known as sensitivity, measures the proportion of true positive predictions among all actual positive instances in the dataset. It helps assess the model's ability to capture all relevant positive instances, which is important in scenarios where missing positive instances is more critical than false alarms.

**F1 Score** is the harmonic mean of precision and recall, providing a balanced measure that considers both false positives and false negatives. It is especially useful when there is an imbalance between the classes or when a balanced view of precision and recall is required for decision-making.

Together, these evaluation metrics play a vital role in assessing the performance of machine learning models, guiding model improvements, and ensuring models meet the desired objectives and requirements of the problem at hand.
